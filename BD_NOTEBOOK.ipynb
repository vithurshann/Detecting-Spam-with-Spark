{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Coursework-Part1-Github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3aJEGS5keqH"
      },
      "source": [
        "# Coursework Part 1: Detecting Spam with Spark\n",
        "\n",
        "These are the tasks for IN432 Big Data coursework 2020, part 1.  \n",
        "\n",
        "This coursework is about classification of e-mail messages as spam or non-spam in Spark. We go through the whole process from loading and preprocessing to training and testing classifiers in a distributed way in Spark. We use the techniques shown in the lextures and labs and a few additional elements will be introduced here, such as the Natural Language ToolKit (NLTK) and some of the preprocessing and machine learning functions that come with Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdFlCqCFkeqL"
      },
      "source": [
        "## Load and prepare the data\n",
        "\n",
        "We will use the lingspam dataset in this coursework (see [http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html) for more information).\n",
        "\n",
        "The next cells only prepare the machine, as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHGQ78mTkeqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "440392f2-6626-4841-d401-15702299f20a"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-hhNOS0keqW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "287249cb-7445-44f2-dec9-76963746f386"
      },
      "source": [
        "%cd\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz > /dev/null\n",
        "!pip install -q findspark\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/root/spark-2.4.5-bin-hadoop2.7\"\n",
        "%cd /content\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "# get a spark context\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "print(sc)\n",
        "# get the context\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "print(spark) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "<pyspark.sql.session.SparkSession object at 0x7f5504d2ca90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nncrHFdwqUmE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "442a73c2-8322-43b4-e9e9-b688db34fd40"
      },
      "source": [
        "# We have a new dataset in directory BigData2020/data/lingspam_public .\n",
        "%cd /content/drive/My Drive/BigData2020/data/lingspam_public \n",
        "# the line above should output should show \"bare  lemm  lemm_stop  readme.txt  stop\"\n",
        "!cat readme.txt\n",
        "# the line above shows the content of the readme file, which explains the structure of the dataset\n",
        "# Lemmatisation is a process similar to stemming"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BigData2020/data/lingspam_public\n",
            "This directory contains the Ling-Spam corpus, as described in the \n",
            "paper:\n",
            "\n",
            "I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, \n",
            "and C.D. Spyropoulos, \"An Evaluation of Naive Bayesian Anti-Spam \n",
            "Filtering\". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), \n",
            "Proceedings of the Workshop on Machine Learning in the New Information \n",
            "Age, 11th European Conference on Machine Learning (ECML 2000), \n",
            "Barcelona, Spain, pp. 9-17, 2000.\n",
            "\n",
            "There are four subdirectories, corresponding to four versions of \n",
            "the corpus:\n",
            "\n",
            "bare: Lemmatiser disabled, stop-list disabled.\n",
            "lemm: Lemmatiser enabled, stop-list disabled.\n",
            "lemm_stop: Lemmatiser enabled, stop-list enabled.\n",
            "stop: Lemmatiser disabled, stop-list enabled.\n",
            "\n",
            "Each one of these 4 directories contains 10 subdirectories (part1, \n",
            "..., part10). These correspond to the 10 partitions of the corpus \n",
            "that were used in the 10-fold experiments. In each repetition, one \n",
            "part was reserved for testing and the other 9 were used for training. \n",
            "\n",
            "Each one of the 10 subdirectories contains both spam and legitimate \n",
            "messages, one message in each file. Files whose names have the form\n",
            "spmsg*.txt are spam messages. All other files are legitimate messages.\n",
            "\n",
            "By obtaining a copy of this corpus you agree to acknowledge the use \n",
            "and origin of the corpus in any published work of yours that makes \n",
            "use of the corpus, and to notify the person below about this work.\n",
            "\n",
            "Ion Androutsopoulos \n",
            "http://www.aueb.gr/users/ion/\n",
            "Ling-Spam corpus last updated: July 17, 2000\n",
            "This file (readme.txt) last updated: July 30, 2003.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb02XcMOkeqq"
      },
      "source": [
        "## Task 1) Read the dataset and create RDDs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7iF1lZukeqt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "4d115858-5560-4f7e-8ca8-980c1912ac3d"
      },
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def makeTestTrainRDDs(pathString):\n",
        "    \"\"\" Takes one of the four subdirectories of the lingspam dataset and returns two RDDs one each for testing and training. \"\"\"\n",
        "    # We should see 10 parts that we can use for creating train and test sets.\n",
        "    p = Path(pathString) # gets a path object representing the current directory path.\n",
        "    dirs = list(p.iterdir()) # get the directories part1 ... part10. \n",
        "#    print(dirs) # Print to check that you have the right directory. You can comment this out when checked. \n",
        "    rddList = [] # create a list for the RDDs\n",
        "    # now create an RDD for each 'part' directory and add them to rddList\n",
        "    print('creating RDDs')\n",
        "    for d in dirs: # iterate through the directories\n",
        "        dir_path = str(d.resolve())\n",
        "        print(dir_path)\n",
        "        rdd = sc.wholeTextFiles(dir_path) #>>> # read the files in the directory \n",
        "        rddList.append(rdd) #>>> append the RDD to the rddList\n",
        "    print('len(rddList)', len(rddList))  # we should now have 10 RDDs in the list # just for testing\n",
        "#   print(rddList[1].take(1)) # just for testing, comment out when it works.\n",
        "\n",
        "    testRDD1 = rddList[9] # set the test set\n",
        "    trainRDD1 = rddList[0] # start the training set from 0 and \n",
        "    # now loop over the range from 1 to 9 (exclusive) to create a union of the remaining RDDs\n",
        "    print('creating RDD union')\n",
        "    for i in range(1, 9):\n",
        "        trainRDD1 = trainRDD1.union(rddList[i]) #>>> create a union of the current and the next \n",
        "            # RDD in the list, so that in the end we have a union of all parts 0-8. (9 is used as test set)\n",
        "    # both RDDs should remove the paths and extensions from the filename. \n",
        "        testRDD2 = testRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2], fn_txt[1]))\n",
        "        trainRDD2 = trainRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2], fn_txt[1]))\n",
        "    return (trainRDD2, testRDD2)\n",
        "\n",
        "# this makes sure we are in the right directory\n",
        "%cd /content/drive/My Drive/BigData2020/data/lingspam_public \n",
        "# this should show \"bare  lemm  lemm_stop  readme.txt  stop\"\n",
        "!ls \n",
        "# the code below is for testing the function makeTestTrainRDDs\n",
        "trainRDD_testRDD = makeTestTrainRDDs('bare') # read from the 'bare' directory - this takes a bit of time\n",
        "(trainRDD, testRDD) = trainRDD_testRDD # unpack the returned tuple\n",
        "print('created the RDDs') # notify the user, so that we can figure out where things went wrong if they do.\n",
        "print('testRDD.count(): ', testRDD.count()) # should be ~289\n",
        "#print('trainRDD.count(): ', trainRDD.count()) # should be ~2604 - commented out to save time as it takes some time to create RDD from all the files\n",
        "print('testRDD.getNumPartitions():', testRDD.getNumPartitions()) # normally 2 on Colab (single machine)\n",
        "print('testRDD.getStorageLevel():', testRDD.getStorageLevel()) # Serialized, 1x Replicated, expected to be (False, False, False, False, 1) \n",
        "print('testRDD.take(1): ', testRDD.take(1)) # should be (filename, [tokens]) \n",
        "rdd1 = testRDD # use this for development in the next tasks "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BigData2020/data/lingspam_public\n",
            "bare  lemm  lemm_stop  readme.txt  stop\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "created the RDDs\n",
            "testRDD.count():  289\n",
            "testRDD.getNumPartitions(): 2\n",
            "testRDD.getStorageLevel(): Serialized 1x Replicated\n",
            "testRDD.take(1):  [('3-1msg1', 'Subject: re : 2 . 882 s - > np np\\n\\n> date : sun , 15 dec 91 02 : 25 : 02 est > from : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 queries > > wlodek zadrozny asks if there is \" anything interesting \" to be said > about the construction \" s > np np \" . . . second , > and very much related : might we consider the construction to be a form > of what has been discussed on this list of late as reduplication ? the > logical sense of \" john mcnamara the name \" is tautologous and thus , at > that level , indistinguishable from \" well , well now , what have we here ? \" . to say that \\' john mcnamara the name \\' is tautologous is to give support to those who say that a logic-based semantics is irrelevant to natural language . in what sense is it tautologous ? it supplies the value of an attribute followed by the attribute of which it is the value . if in fact the value of the name-attribute for the relevant entity were \\' chaim shmendrik \\' , \\' john mcnamara the name \\' would be false . no tautology , this . ( and no reduplication , either . )\\n')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUKltxq5keqw"
      },
      "source": [
        "## Task 2) Tokenize and remove punctuation\n",
        "\n",
        "Python [Natural Language Toolkit](http://www.nltk.org) (*NLTK*) to do the tokenization. We use the NLTK function word_tokenize, see here for a code example: [http://www.nltk.org/book/ch03.html](http://www.nltk.org/book/ch03.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AB_nfmhYkeqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "225eb8a2-ec55-4cb0-d499-8dc1b392ece7"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\" Apply the nltk.word_tokenize() method to our text, return the token list. \"\"\"\n",
        "    nltk.download('punkt') #  loads the standard NLTK tokenizer model \n",
        "    # it is important that this is done here in the function, as it needs to be done on every worker.\n",
        "    # If we do the download outside a this function, it would only be executed on the driver     \n",
        "    return nltk.word_tokenize(text)\n",
        "    \n",
        "def removePunctuation(tokens):\n",
        "    \"\"\" Remove punctuation characters from all tokens in a provided list. \"\"\"\n",
        "    # this will remove all punctiation from string s: re.sub('[()\\[\\],.?!\";_]','', s)\n",
        "    tokens2 = [re.sub('[()\\[\\],.?!\";_]','', s)for s in tokens]\n",
        "    return tokens2\n",
        "    \n",
        "def prepareTokenRDD(fn_txt_RDD):\n",
        "    \"\"\" Take an RDD with (filename, text) elements and transform it into a (filename, [token ...]) RDD without punctuation characters. \"\"\"\n",
        "    rdd_vals2 = fn_txt_RDD.values() # It's convenient to process only the values. \n",
        "    rdd_vals3 = rdd_vals2.map(tokenize) # Create a tokenised version of the values by mapping\n",
        "    rdd_vals4 = rdd_vals3.map(removePunctuation) # remove punctuation from the values\n",
        "    rdd_kv = fn_txt_RDD.keys().zip(rdd_vals4) # we zip the two RDDs together \n",
        "    # i.e. produce tuples with one item from each RDD.\n",
        "    # This works because we have only applied mappings to the values, \n",
        "    # therefore the items in both RDDs are still aligned.\n",
        "    # >>> now remove any empty strings (i.e. length 0) that we may have \n",
        "    # created by removing punctuation, and resulting entries without words left.\n",
        "    rdd_kvr = rdd_kv.map(lambda x: (x[0], [token for token in x[1] if len(token)>0]))\n",
        "    rdd_kvrf = rdd_kvr.filter(lambda x: len(x[1])>0)\n",
        "    \n",
        "    return rdd_kvrf \n",
        "\n",
        "rdd2 = prepareTokenRDD(rdd1) # Use a small RDD for testing.\n",
        "print(rdd2.take(1)) # For checking result of task 2. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('3-1msg1', ['Subject', ':', 're', ':', '2', '882', 's', '-', '>', 'np', 'np', '>', 'date', ':', 'sun', '15', 'dec', '91', '02', ':', '25', ':', '02', 'est', '>', 'from', ':', 'michael', '<', 'mmorse', '@', 'vm1', 'yorku', 'ca', '>', '>', 'subject', ':', 're', ':', '2', '864', 'queries', '>', '>', 'wlodek', 'zadrozny', 'asks', 'if', 'there', 'is', '``', 'anything', 'interesting', '``', 'to', 'be', 'said', '>', 'about', 'the', 'construction', '``', 's', '>', 'np', 'np', '``', 'second', '>', 'and', 'very', 'much', 'related', ':', 'might', 'we', 'consider', 'the', 'construction', 'to', 'be', 'a', 'form', '>', 'of', 'what', 'has', 'been', 'discussed', 'on', 'this', 'list', 'of', 'late', 'as', 'reduplication', 'the', '>', 'logical', 'sense', 'of', '``', 'john', 'mcnamara', 'the', 'name', '``', 'is', 'tautologous', 'and', 'thus', 'at', '>', 'that', 'level', 'indistinguishable', 'from', '``', 'well', 'well', 'now', 'what', 'have', 'we', 'here', '``', 'to', 'say', 'that', \"'\", 'john', 'mcnamara', 'the', 'name', \"'\", 'is', 'tautologous', 'is', 'to', 'give', 'support', 'to', 'those', 'who', 'say', 'that', 'a', 'logic-based', 'semantics', 'is', 'irrelevant', 'to', 'natural', 'language', 'in', 'what', 'sense', 'is', 'it', 'tautologous', 'it', 'supplies', 'the', 'value', 'of', 'an', 'attribute', 'followed', 'by', 'the', 'attribute', 'of', 'which', 'it', 'is', 'the', 'value', 'if', 'in', 'fact', 'the', 'value', 'of', 'the', 'name-attribute', 'for', 'the', 'relevant', 'entity', 'were', \"'\", 'chaim', 'shmendrik', \"'\", \"'\", 'john', 'mcnamara', 'the', 'name', \"'\", 'would', 'be', 'false', 'no', 'tautology', 'this', 'and', 'no', 'reduplication', 'either'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny-9B3363jES"
      },
      "source": [
        "**Question:** why should this be filtering done after zipping the keys and values together?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8TSwSyL3w5d"
      },
      "source": [
        "**Answer:** Because it is more efficient to filter after zipping the keys and values together. When the RDD is split, two RDD's are created (KeysRDD and ValuesRDD). When filtering, it may remove items from ValuesRDD and if it is done before zipping then you will have to manually find the KeysRDD (of the items that were removed from ValuesRDD) and remove it and reassign the rest which will be time consuming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9HgRAEGkeq0"
      },
      "source": [
        "## Task 3) Creating normalised TF.IDF vectors of defined dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo_YQKAokeq0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cbe6b17-a5eb-4f88-8561-bfd14267504b"
      },
      "source": [
        "# use the hashing trick to create a fixed-size vector from a word list\n",
        "def hashing_vectorize(text, N): # arguments: the list and the size of the output vector\n",
        "    v = [0] * N  # create vector of 0s\n",
        "    for word in text: # iterate through the words \n",
        "      hashValue = hash(word)\n",
        "      v[hashValue%N] += 1\n",
        "    return v # return hashed word vector\n",
        "\n",
        "from pyspark.mllib.feature import IDF, Normalizer\n",
        "\n",
        "def normTFIDF(fn_tokens_RDD, vecDim):\n",
        "    keysRDD = fn_tokens_RDD.keys()\n",
        "    tokensRDD = fn_tokens_RDD.values()\n",
        "    tfVecRDD = tokensRDD.map(lambda tokens: hashing_vectorize(tokens, vecDim)) \n",
        "    idf = IDF() # create IDF object\n",
        "    idfModel = idf.fit(tfVecRDD) # calculate IDF values\n",
        "    tfIdfRDD = idfModel.transform(tfVecRDD) # 2nd pass needed (see lecture slides), transforms RDD\n",
        "    norm = Normalizer()\n",
        "    normTfIdfRDD = norm.transform(tfIdfRDD)\n",
        "    zippedRDD = keysRDD.zip(normTfIdfRDD)\n",
        "    return zippedRDD\n",
        "\n",
        "testDim = 10 # too small for good accuracy, but OK for testing\n",
        "rdd3 = normTFIDF(rdd2, testDim) # test our normTFIDF function\n",
        "print(rdd3.take(1)) # we should now have tuples with ('filename', [N-dim vector])\n",
        "# e.g. [('3-1msg1', DenseVector([0.0, 0.1629, 0.6826, 0.0, 0.0, 0.0, 0.4017, 0.3258, 0.3133, 0.3766]))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('3-1msg1', DenseVector([0.0, 0.1629, 0.6826, 0.0, 0.0, 0.0, 0.4017, 0.3258, 0.3133, 0.3766]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-xzB6hrkeq5"
      },
      "source": [
        "## Task 4) Create LabeledPoints \n",
        "\n",
        "Determine whether the file is spam (i.e. the filename contains ’spmsg’) and replace the filename by a 1 (spam) or 0 (non-spam) accordingly. Use `RDD.map()` to create an RDD of LabeledPoint objects. See here [http://spark.apache.org/docs/2.4.5/mllib-linear-methods.html#logistic-regression](http://spark.apache.org/docs/2.4.5/mllib-linear-methods.html#logistic-regression) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pixiedust": {
          "displayParams": {
            "handlerId": "tableView"
          }
        },
        "id": "wF9BDmnEkeq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e3a61eb-53b6-4b56-fe2b-c158a2ed455c"
      },
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "# create labelled points of vector size N out of an RDD with normalised (filename, td.idf-vector) items\n",
        "def makeLabeledPoints(fn_vec_RDD): # RDD and N needed \n",
        "    # we determine the true class as encoded in the filename and represent as 1 (spam) or 0 (good) \n",
        "    cls_vec_RDD = fn_vec_RDD.map(lambda x: (1, x[1]) if x[0].startswith('spmsg') else (0, x[1]))\n",
        "    # now we can create the LabeledPoint objects with (class, vector) arguments\n",
        "    lp_RDD = cls_vec_RDD.map(lambda cls_vec: LabeledPoint(cls_vec[0], cls_vec[1]) ) \n",
        "    return lp_RDD \n",
        "\n",
        "# for testing\n",
        "testLpRDD = makeLabeledPoints(rdd3)\n",
        "print(testLpRDD.take(1))\n",
        "# should look similar to this: [LabeledPoint(0.0, [0.0,0.16290896085571283,0.6826175329317583,0.0,0.0,0.0,0.40170165983309447,0.32581792171142565,0.3132864631840631,0.3765953060935261])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LabeledPoint(0.0, [0.0,0.16290896085571283,0.6826175329317583,0.0,0.0,0.0,0.40170165983309447,0.32581792171142565,0.3132864631840631,0.3765953060935261])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuLuz17zkeq8"
      },
      "source": [
        "## Task 5) Complete the preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9F8Kl3ukeq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "bc9434e8-d0bb-4ff5-bda7-e2afbbc8490f"
      },
      "source": [
        "# now we can apply the preprocessing chain to the data loaded in task 1 \n",
        "# N is for controlling the vector size\n",
        "def preprocess(rawRDD, N):\n",
        "    \"\"\" take a (filename,text) RDD and transform into LabelledPoint objects \n",
        "        with class labels and a TF.IDF vector with N dimensions. \n",
        "    \"\"\"\n",
        "# tasks 2, 3 and 4 \n",
        "    tokenRDD = prepareTokenRDD(rawRDD) \n",
        "    tfIdfRDD = normTFIDF(tokenRDD,N) \n",
        "    lpRDD = makeLabeledPoints(tfIdfRDD) \n",
        "    return lpRDD # return RDD with LabeledPoints\n",
        "\n",
        "# and with this we can start the whole process from a directory, N is again the vector size\n",
        "def loadAndPreprocess(directory, N):\n",
        "    \"\"\" load lingspam data from a directory and create a training and test set of preprocessed data \"\"\"\n",
        "    # read from the directory using the function created in task 1\n",
        "    # unpack the returned tuple\n",
        "    trainRDD_testRDD = makeTestTrainRDDs(directory)\n",
        "    trainRDD, testRDD = trainRDD_testRDD\n",
        "    return (preprocess(trainRDD, N), preprocess(testRDD, N)) # apply the preprocessing function defined above\n",
        "\n",
        "trainLpRDD = preprocess(trainRDD, testDim) # prepare the training data\n",
        "print(trainLpRDD.take(1)) # should look similar to previous cell's output\n",
        "\n",
        "train_test_LpRDD = loadAndPreprocess('lemm', 100) # let's re-run with another vector size\n",
        "(trainLpRDD, testLpRDD) = train_test_LpRDD\n",
        "print(testLpRDD.take(1))\n",
        "print(trainLpRDD.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LabeledPoint(0.0, [0.5431144491961283,0.39172111622608674,0.35540711046827655,0.0,0.0,0.19045658048349204,0.3625647437051716,0.3592848102947122,0.2847076534848611,0.2177065059127546])]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "[LabeledPoint(0.0, [0.04138897380669189,0.06905413571056576,0.09383464193692988,0.05512201563970116,0.07984722995624746,0.18013185637317497,0.0,0.20822625541036702,0.07069696035842367,0.13547187640721076,0.0,0.06707475330330137,0.0,0.13709891148926243,0.06970516550511324,0.13157111094298884,0.0,0.018695161508859446,0.06004395212439166,0.14080467630467874,0.052631737455157514,0.031066880462049386,0.0,0.0,0.0,0.08404971781482777,0.11493762390908602,0.07386494891693535,0.0,0.0,0.0,0.0,0.0,0.0,0.1199394317427922,0.0,0.17730764663842596,0.0,0.10528784819161091,0.0,0.0,0.3070050802808594,0.053543655934717986,0.1723678189276023,0.0,0.0,0.08942769316152013,0.12008790424878332,0.10885318879037179,0.07777804020231302,0.05172388098674139,0.07471311868383705,0.030641126463577917,0.32355902487976923,0.0,0.0563041510483258,0.053543655934717986,0.0,0.0,0.09607437260966498,0.0,0.0,0.0,0.0,0.27487269420506455,0.052819036192073655,0.0,0.0,0.0,0.23266189576249866,0.023830140138709842,0.35771077264608053,0.0,0.08286496285267891,0.1535025401404297,0.07984722995624746,0.2061545206537984,0.0,0.0,0.11370611901324568,0.06530066111419289,0.0,0.0,0.0,0.11633094788124933,0.0,0.03603818643395556,0.07881000919268093,0.0,0.08420783056445955,0.0,0.130327077944359,0.12120930152160075,0.06578555547149442,0.0,0.05083248990813509,0.0,0.0,0.0,0.0])]\n",
            "[LabeledPoint(0.0, [0.039040846770928436,0.022674391863931637,0.05382002095379722,0.04150467469020853,0.05635234101139964,0.07915689212662691,0.046942949727828004,0.0287849031640231,0.03732873224682722,0.06169339060751134,0.0,0.3386496584233161,0.051662386799475636,0.05238069252089395,0.01928444193006618,0.08550557953919574,0.036354303516330747,0.019223852681304707,0.07825827311556759,0.3820547208945783,0.07382113653790466,0.04674636550685318,0.026385630708875635,0.01900984469827045,0.014343913662492585,0.2747689642505897,0.35728057051386114,0.03455987510596566,0.03972400848867156,0.08507392682477877,0.09385206654244857,0.019495200059093134,0.06211037183650699,0.11093364143285014,0.04129569637200316,0.017143434484527917,0.14651608662963908,0.06289599070490164,0.10479548039220947,0.010594392412826819,0.11334341460224598,0.20525220067163827,0.07984534968743723,0.05251668659182594,0.009994237667291003,0.04007975896016933,0.10719440894217452,0.06263713015836268,0.16272355677620967,0.0582509418324443,0.07119877145420032,0.041655958381150916,0.019809573544071663,0.09252359064234067,0.03877380701163336,0.05052704169569673,0.11502588164224686,0.1493149289873089,0.0482765902530004,0.09054485361876083,0.27801449661245586,0.02026044039029053,0.012699723766866476,0.05150762156260405,0.13797376305844428,0.04515418034146933,0.057351790020233655,0.017143434484527917,0.014077859344051565,0.06487721675568311,0.035523002917341055,0.06116921813829097,0.014611503052370299,0.022229519150869643,0.0,0.12410413598204406,0.03397955702473912,0.027368337549214476,0.03637136019169334,0.07923831530917867,0.025351121998979348,0.16177813892965795,0.023820979764927672,0.0,0.10863641226342821,0.01512668292543555,0.05856741249628033,0.13488700804095688,0.018212038708586394,0.04067992482656352,0.06336799353788941,0.0,0.08374456966190376,0.065316896070828,0.0,0.04841316230630353,0.07087214934518402,0.0425958118571764,0.055235546680043844,0.02150252839134035])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdwyXqu4keq_"
      },
      "source": [
        "## Task 6) Train some classifiers \n",
        "\n",
        "Use the `LabeledPoint` objects to train a classifier, specifically *Logistic Regression*, *Naive Bayes*, and *Support Vector Machine*. Calculate the accuracy of the model on the training set (by dividing the number of correctly recognised messages by the total number of messages, again, following the example [http://spark.apache.org/docs/2.4.5/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.4.5/ml-classification-regression.html#logistic-regression) and the documentation for the classifiers [LogisticRegressionWithLBFGS](http://spark.apache.org/docs/2.4.5/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS), [NaiveBayes](http://spark.apache.org/docs/2.4.5/api/python/pyspark.mllib.html#pyspark.mllib.classification.NaiveBayes), [SVMWithSGD](http://spark.apache.org/docs/2.4.5/api/python/pyspark.mllib.html#pyspark.mllib.classification.SVMWithSGD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljcgl0mLkeq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a583ab83-2d88-4d3a-c271-0a65d4f0c96d"
      },
      "source": [
        "from pyspark.mllib.classification import NaiveBayes, LogisticRegressionWithLBFGS, SVMWithSGD\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# train the model with a LabeledPoint RDD.\n",
        "def trainModel(lpRDD):\n",
        "    \"\"\" Train 3 classifier models on the given RDD with LabeledPoint objects. A list of trained model is returned. \"\"\"\n",
        "    # Train a classifier model.\n",
        "    print('Starting to train the model') # give some immediate feedback\n",
        "    model1 = LogisticRegressionWithLBFGS.train(lpRDD) # this is the best model\n",
        "    print('Trained LR (model1)')\n",
        "    #print('type(model1)')\n",
        "    model2 = NaiveBayes.train(lpRDD) # doesn't work well\n",
        "    print('Trained NB (model2)')\n",
        "    #print(type(model2))\n",
        "    model3 = SVMWithSGD.train(lpRDD) # or this ...\n",
        "    print('Trained SVM (model3)')\n",
        "    return [model1, model2, model3]\n",
        "\n",
        "def testModel(model, lpRDD):\n",
        "    \"\"\" Tests the classification accuracy of the given model on the given RDD with LabeledPoint objects. \"\"\"\n",
        "    lpRDD.persist(StorageLevel.MEMORY_ONLY)\n",
        "    # Make prediction and evaluate training set accuracy\n",
        "    # Get the prediction and the ground truth label\n",
        "    predictionAndLabel = lpRDD.map(lambda p: (model.predict(p.features), p.label)) # get the prediction and ground truth (label) for each item\n",
        "    correct = predictionAndLabel.filter(lambda xv: xv[0] == xv[1]).count() # count the correct predictions \n",
        "    #calculate the accuracy \n",
        "    accuracy = correct/predictionAndLabel.count()\n",
        "    print('Accuracy {:.1%} (data items: {}, correct: {})'.format(accuracy, lpRDD.count(), correct)) # report to console\n",
        "    return accuracy # and return the value  \n",
        "\n",
        "models = trainModel(trainLpRDD) # just for testing\n",
        "testModel(models[2], trainLpRDD) # just for testing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8337173579109063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TeHFXcTkerC"
      },
      "source": [
        "## Task 7) Automate training and testing\n",
        "\n",
        "Automate the whole process from reading the files, through preprocessing, and training up to evaluating the models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj_Xh5oekerD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "17d7f875-baaa-409e-c07d-467fb55cd4c6"
      },
      "source": [
        "# this function combines the previous two functions\n",
        "# this method should take RDDs with LabeledPoints\n",
        "def trainTestModel(trainRDD, testRDD):\n",
        "    \"\"\" Trains 3 models and tests them on training and test data. Returns a matrix with the training and testing (rows) accuracy values for all models (columns). \"\"\"\n",
        "    # train models on the training set\n",
        "    models = trainModel(trainRDD)\n",
        "    results = [[], []] # matrix for 2 modes (training/test) vs n models (currently 3)\n",
        "    for mdl in models:\n",
        "        print('Training')\n",
        "        # test the model on the training set\n",
        "        results[0].append(testModel(mdl, trainRDD))\n",
        "        print('Testing')\n",
        "        # test the model on the test set\n",
        "        results[1].append(testModel(mdl, testRDD))\n",
        "    return results\n",
        "\n",
        "def trainTestFolder(folder,N):\n",
        "    \"\"\" Reads data from a folder, preproceses the data, and trains and evaluates models on it. \"\"\"\n",
        "    print('Start loading and preprocessing') \n",
        "    train_test_LpRDD = loadAndPreprocess(folder,N) # create the RDDs\n",
        "    print('Finished loading and preprocessing')\n",
        "    (trainLpRDD, testLpRDD) = train_test_LpRDD # unpack the RDDs \n",
        "    return trainTestModel(trainLpRDD,testLpRDD) # train and test\n",
        "\n",
        "trainTestFolder('lemm', 1000) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 97.2% (data items: 289, correct: 281)\n",
            "Training\n",
            "Accuracy 94.4% (data items: 2604, correct: 2459)\n",
            "Testing\n",
            "Accuracy 92.7% (data items: 289, correct: 268)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0, 0.9443164362519201, 0.8337173579109063],\n",
              " [0.972318339100346, 0.9273356401384083, 0.8339100346020761]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzcNex1NkerF"
      },
      "source": [
        "## Task 8) Run experiments \n",
        "\n",
        "We have now a single function that allows us to vary the vector size easily. Test vector sizes 3, 30, 300, 3000, 30000 and examine the effect on the classification accuracy in Experiment 1.\n",
        "\n",
        "Use the function from Task 7) to test different data types. The dataset has raw text in folder `bare`, lemmatised text in  `lemm` (similar to stemming, reduces to basic word forms), `stop` (with stopwords removed), and `lemm_stop` (lemmatised and stopwords removed). Test how the classification accuracy differs for these four data types in Experiment 2. Collected the results in a data structure for later analyis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HmW_CpetkerG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b6d13b8-8af4-4b10-fe72-83ded9709824"
      },
      "source": [
        "folder = 'bare'\n",
        "N = [3, 30, 300, 3000, 30000]\n",
        "print('\\nEXPERIMENT 1: Testing different vector sizes')\n",
        "results_vectorsizes = []\n",
        "for n in N:\n",
        "    print('N = {}'.format(n))\n",
        "    result = {'n': n, 't': folder}\n",
        "    result['acc'] = trainTestFolder(folder, n)\n",
        "    results_vectorsizes.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EXPERIMENT 1: Testing different vector sizes\n",
            "N = 3\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "N = 30\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 85.8% (data items: 2604, correct: 2233)\n",
            "Testing\n",
            "Accuracy 84.8% (data items: 289, correct: 245)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "N = 300\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 93.1% (data items: 289, correct: 269)\n",
            "Training\n",
            "Accuracy 86.1% (data items: 2604, correct: 2243)\n",
            "Testing\n",
            "Accuracy 84.8% (data items: 289, correct: 245)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "N = 3000\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 96.9% (data items: 289, correct: 280)\n",
            "Training\n",
            "Accuracy 97.4% (data items: 2604, correct: 2536)\n",
            "Testing\n",
            "Accuracy 96.2% (data items: 289, correct: 278)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "N = 30000\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 97.2% (data items: 289, correct: 281)\n",
            "Training\n",
            "Accuracy 86.3% (data items: 2604, correct: 2246)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2172)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ll1Ag9BLqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e8adc9f-f2eb-4e5f-ec63-b63a47d72a4b"
      },
      "source": [
        "n = 3000\n",
        "typeFolders = ['bare', 'stop', 'lemm', 'lemm_stop']\n",
        "print('EXPERIMENT 2: Testing different data types')\n",
        "results_preprocessing = []\n",
        "for folder in typeFolders:\n",
        "    print('Path = {}'.format(folder))\n",
        "    result = {'n': n, 't': folder}\n",
        "    result['acc'] = trainTestFolder(folder, n)\n",
        "    results_preprocessing.append(result)\n",
        "\n",
        "# Add comments on the performance in a cell below. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXPERIMENT 2: Testing different data types\n",
            "Path = bare\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 96.9% (data items: 289, correct: 280)\n",
            "Training\n",
            "Accuracy 97.4% (data items: 2604, correct: 2536)\n",
            "Testing\n",
            "Accuracy 96.2% (data items: 289, correct: 278)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Path = stop\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/stop/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 97.6% (data items: 289, correct: 282)\n",
            "Training\n",
            "Accuracy 96.7% (data items: 2604, correct: 2518)\n",
            "Testing\n",
            "Accuracy 95.2% (data items: 289, correct: 275)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Path = lemm\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 97.6% (data items: 289, correct: 282)\n",
            "Training\n",
            "Accuracy 97.4% (data items: 2604, correct: 2535)\n",
            "Testing\n",
            "Accuracy 95.5% (data items: 289, correct: 276)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Path = lemm_stop\n",
            "Start loading and preprocessing\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm_stop/part1\n",
            "len(rddList) 10\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
            "Testing\n",
            "Accuracy 96.9% (data items: 289, correct: 280)\n",
            "Training\n",
            "Accuracy 96.9% (data items: 2604, correct: 2523)\n",
            "Testing\n",
            "Accuracy 94.8% (data items: 289, correct: 274)\n",
            "Training\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9n4ecn_Z3_k"
      },
      "source": [
        "**Experiment** **1**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Initial observation shows that there is a difference between the training and testing accuracy which is expected. However, when vector size is 3, training and testing return the accuracy of 83.4 for all the models so it is safe to assume that the models are predicting all as no spam(major class) and due to the high imbalance, they're managing an accuracy of 83.4. When vector size is 30(low again), there is not much of a difference between the training and testing accuracy. But as vector size increases, there is changes in the performance. \n",
        "\n",
        "*Logistic Regression*\n",
        "- Performance imporves as vector size increases as it went from 83.4 (VectorSize = 3) testing accurancy to 97.2(VectorSize =30000) testing accuracy. \n",
        "\n",
        "*Naive Bayes*\n",
        "- Naive Bayes performance increases as vector size is increased from 3 to 3000, similar to Linear Regression. However, when the vector size is 30000, accuracy drops 83.4 form 96.2 which could suggest that after a certain point we are adding noise rather adding informative features.\n",
        "\n",
        "*SVM*\n",
        "- SVM is very ineffective as the changes to vector size didnt cotribute towards the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1L8rqZaA5pp"
      },
      "source": [
        "**Experiment** **2**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Logistic Regression*\n",
        "- Performed excellently on the training with a 100% accuracy for all the data types which may suggest it is overfitting but the lowest testing accuracy is 96.9 and highest is 97.6 which shows that is not the case and performes really well. \n",
        "- StopWords removal and Lemmatization improved the accuracy but doing both reduced the accuracy which may be due to lemmatization resulting in more stopwords which the ends up being removed.\n",
        "\n",
        "*Naive Bayes*\n",
        "- Naive Bayes performs really  on all data types with best performance accuracy on \"bare\" which suggests pre-processing doesnt improve the accuracy of the model.\n",
        "\n",
        "*SVM*\n",
        "- No change in performance even after pre-processing. \n",
        "\n",
        "Logistic Regression is the best performing model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANHk7SeMiTFc"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}